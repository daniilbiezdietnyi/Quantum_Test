# -*- coding: utf-8 -*-
"""Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZLMDhqVtaDzmJtuFXIxJIq1NONP3f7zC
"""

!pip install transformers datasets tokenizers seqeval -q
!pip install evaluate

import json
import random
import datasets
import numpy as np
import evaluate
from transformers import BertTokenizerFast
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification
from transformers import TrainingArguments, Trainer

def load_dataset(file_path):
    """
    Function for loading json file by specified file_path
    """
    with open(file_path, 'r', encoding='utf-8') as json_file:
        dataset = json.load(json_file)
    return dataset

def tokenize_and_align_labels(examples, label_all_tokens=False):
    """
    Tokenizes input text and aligns named entity recognition (NER) labels with the tokens,
    handling subwords based on the label_all_tokens flag.
    Returns tokenized inputs with aligned labels.
    """
    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, padding='max_length', is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples['ner_tags']):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get the word IDs of the tokens
        previous_word_idx = None
        label_ids = []

        for word_idx in word_ids:
            if word_idx is None:
                # Special tokens like [CLS] and [SEP] don't correspond to any label
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                # Check if word_idx is valid
                if word_idx < len(label):  # Ensure it is within range
                    label_ids.append(label[word_idx])
                else:
                    print(f"Warning: word_idx {word_idx} out of range for example {i}.")
                    label_ids.append(-100)
            else:
                # For subwords, we either keep the same label or ignore them (-100)
                if label_all_tokens:
                    label_ids.append(label[word_idx])
                else:
                    label_ids.append(-100)

            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

def compute_metrics(eval_preds):
    """
    Returns metrics for training
    """
    pred_logits, labels = eval_preds
    pred_logits = np.argmax(pred_logits, axis=2)

    # Use list comprehensions to create filtered predictions and true labels
    predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(pred_logits, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(pred_logits, labels)
    ]

    # Ensure predictions are non-empty before computing metrics
    if predictions and true_labels:
        results = metric.compute(predictions=predictions, references=true_labels)
    else:
        results = {"overall_precision": 0, "overall_recall": 0, "overall_f1": 0, "overall_accuracy": 0}

    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

file_path = '/content/result_data.json'
data = load_dataset(file_path)
random.shuffle(data)

tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

dataset = datasets.Dataset.from_list(data)
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

model = AutoModelForTokenClassification.from_pretrained("bert-base-uncased", num_labels=3)

data_collator = DataCollatorForTokenClassification(tokenizer)

# Load seqeval metric for evaluation
metric = evaluate.load("seqeval")

# Define the list of labels
label_list = ['O', 'B-MOUNT', 'I-MOUNT']

args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    logging_strategy="steps",
    report_to="none",
)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets.select(range(350)),
    eval_dataset=tokenized_datasets.select(range(350, len(tokenized_datasets))),
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

model.save_pretrained("ner_mountains_resulting")

#Creating dictionaries for config
id2label = {
    str(i): label for i,label in enumerate(label_list)
}
label2id = {
    label: str(i) for i,label in enumerate(label_list)
}

config = json.load(open("ner_mountains_resulting/config.json"))
config["id2label"] = id2label
config["label2id"] = label2id
json.dump(config, open("ner_mountains_resulting/config.json","w"))